# main.py
import streamlit as st
import logging
from typing import Optional
from dotenv import load_dotenv
from seed_data import seed_milvus, seed_milvus_live
from agent import get_retriever as get_openai_retriever, get_llm_and_agent as get_openai_agent
from local_ollama import get_retriever as get_ollama_retriever, get_llm_and_agent as get_ollama_agent
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from threading import Lock

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Thread safety for session state
session_lock = Lock()

class ChatInterface:
    """Enhanced chat interface management"""
    def __init__(self):
        self.msgs = StreamlitChatMessageHistory(key="langchain_messages")
        
    def initialize_chat(self):
        """Initialize chat with welcome message"""
        with session_lock:
            if "messages" not in st.session_state:
                st.session_state.messages = [
                    {"role": "assistant", "content": "T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?"}
                ]
                self.msgs.add_ai_message("T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?")
                
    def display_messages(self):
        """Display chat messages"""
        for msg in st.session_state.messages:
            role = "assistant" if msg["role"] == "assistant" else "human"
            st.chat_message(role).write(msg["content"])
            
    def handle_user_input(self, agent_executor):
        """Process user input with error handling"""
        if prompt := st.chat_input("H√£y h·ªèi t√¥i ƒëi·ªÅu g√¨ ƒë√≥..."):
            try:
                # Display user message
                with session_lock:
                    st.session_state.messages.append({"role": "human", "content": prompt})
                st.chat_message("human").write(prompt)
                self.msgs.add_user_message(prompt)

                # Process and display response
                with st.chat_message("assistant"):
                    try:
                        with st.spinner("ƒêang t√¨m ki·∫øm th√¥ng tin..."):
                            # Setup callback
                            st_callback = StreamlitCallbackHandler(st.container())
                            
                            # Get chat history
                            chat_history = [
                                {"role": msg["role"], "content": msg["content"]}
                                for msg in st.session_state.messages[:-1]
                            ]

                            # Get AI response
                            response = agent_executor.invoke(
                                {
                                    "input": prompt,
                                    "chat_history": chat_history
                                },
                                {"callbacks": [st_callback]}
                            )
                            
                            output = response["output"]
                            
                            # Update messages
                            with session_lock:
                                st.session_state.messages.append(
                                    {"role": "assistant", "content": output}
                                )
                            self.msgs.add_ai_message(output)
                            st.write(output)
                            
                    except Exception as e:
                        error_msg = "Xin l·ªói, t√¥i ƒëang g·∫∑p s·ª± c·ªë. Vui l√≤ng th·ª≠ l·∫°i sau."
                        st.error(error_msg)
                        logger.error(f"Agent execution error: {str(e)}")
                        
            except Exception as e:
                st.error("C√≥ l·ªói x·∫£y ra. Vui l√≤ng th·ª≠ l·∫°i.")
                logger.error(f"Message handling error: {str(e)}")

class SidebarManager:
    """Enhanced sidebar management"""
    def setup_sidebar(self) -> str:
        """Setup sidebar with improved UI/UX"""
        with st.sidebar:
            st.title("‚öôÔ∏è C·∫•u h√¨nh")
            
            # Embeddings Model Selection
            st.header("üî§ Embeddings Model")
            embeddings_choice = st.radio(
                "Ch·ªçn Embeddings Model:",
                ["OpenAI", "Ollama"],
                help="Ch·ªçn model ƒë·ªÉ t·∫°o embeddings cho documents"
            )
            use_ollama_embeddings = (embeddings_choice == "Ollama")
            
            # Data Source Configuration
            st.header("üìö Ngu·ªìn d·ªØ li·ªáu")
            data_source = st.radio(
                "Ch·ªçn ngu·ªìn d·ªØ li·ªáu:",
                ["File Local", "URL tr·ª±c ti·∫øp"]
            )
            
            # Handle data source
            if data_source == "File Local":
                self._handle_local_file(use_ollama_embeddings)
            else:
                self._handle_url_input(use_ollama_embeddings)
            
            # AI Model Selection
            st.header("ü§ñ Model AI")
            model_choice = st.radio(
                "Ch·ªçn AI Model:",
                ["GitHub AI", "Ollama (Local)"],
                help="Ch·ªçn model AI ƒë·ªÉ x·ª≠ l√Ω c√¢u h·ªèi"
            )
            
            return model_choice
            
    def _handle_local_file(self, use_ollama_embeddings: bool):
        """Handle local file input"""
        collection_name = st.text_input(
            "T√™n collection:",
            "data_test",
            help="Nh·∫≠p t√™n collection trong Milvus"
        )
        
        filename = st.text_input(
            "T√™n file JSON:",
            "stack.json",
            help="Nh·∫≠p t√™n file JSON ch·ª©a d·ªØ li·ªáu"
        )
        
        directory = st.text_input(
            "Th∆∞ m·ª•c ch·ª©a file:",
            "data",
            help="Nh·∫≠p ƒë∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a file"
        )
        
        if st.button("T·∫£i d·ªØ li·ªáu t·ª´ file", help="Click ƒë·ªÉ b·∫Øt ƒë·∫ßu t·∫£i d·ªØ li·ªáu"):
            self._process_local_file(
                collection_name,
                filename,
                directory,
                use_ollama_embeddings
            )
            
    def _process_local_file(
        self,
        collection_name: str,
        filename: str,
        directory: str,
        use_ollama: bool
    ):
        """Process local file loading"""
        if not collection_name:
            st.error("Vui l√≤ng nh·∫≠p t√™n collection!")
            return
            
        try:
            with st.spinner("ƒêang t·∫£i d·ªØ li·ªáu..."):
                seed_milvus(
                    'http://localhost:19530',
                    collection_name,
                    filename,
                    directory,
                    use_ollama=use_ollama
                )
                st.success(f"ƒê√£ t·∫£i d·ªØ li·ªáu v√†o collection '{collection_name}'!")
        except Exception as e:
            st.error(f"L·ªói khi t·∫£i d·ªØ li·ªáu: {str(e)}")
            logger.error(f"Local file processing error: {str(e)}")
            
    def _handle_url_input(self, use_ollama_embeddings: bool):
        """Handle URL input"""
        collection_name = st.text_input(
            "T√™n collection:",
            "data_test_live",
            help="Nh·∫≠p t√™n collection trong Milvus"
        )
        
        url = st.text_input(
            "Nh·∫≠p URL:",
            "https://www.stack-ai.com/docs",
            help="Nh·∫≠p URL ƒë·ªÉ crawl d·ªØ li·ªáu"
        )
        
        if st.button("Crawl d·ªØ li·ªáu", help="Click ƒë·ªÉ b·∫Øt ƒë·∫ßu crawl"):
            self._process_url(collection_name, url, use_ollama_embeddings)
            
    def _process_url(self, collection_name: str, url: str, use_ollama: bool):
        """Process URL crawling"""
        if not collection_name:
            st.error("Vui l√≤ng nh·∫≠p t√™n collection!")
            return
            
        try:
            with st.spinner("ƒêang crawl d·ªØ li·ªáu..."):
                seed_milvus_live(
                    url,
                    'http://localhost:19530',
                    collection_name,
                    'stack-ai',
                    use_ollama=use_ollama
                )
                st.success(f"ƒê√£ crawl d·ªØ li·ªáu v√†o collection '{collection_name}'!")
        except Exception as e:
            st.error(f"L·ªói khi crawl d·ªØ li·ªáu: {str(e)}")
            logger.error(f"URL processing error: {str(e)}")

class App:
    """Main application class"""
    def __init__(self):
        self.chat_interface = ChatInterface()
        self.sidebar_manager = SidebarManager()
        
    def setup_page(self):
        """Configure page settings"""
        st.set_page_config(
            page_title="AI Assistant",
            page_icon="üí¨",
            layout="wide",
            initial_sidebar_state="expanded"
        )
        
    def initialize(self):
        """Initialize application"""
        load_dotenv()
        self.setup_page()
        
    def setup_chat_interface(self, model_choice: str):
        """Setup main chat interface"""
        st.title("üí¨ AI Assistant")
        
        # Dynamic caption based on model
        caption = ("üöÄ Tr·ª£ l√Ω AI ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi LangChain v√† " + 
                  ("GitHub AI" if model_choice == "GitHub AI" else "Ollama LLaMA2"))
        st.caption(caption)
        
        # Initialize chat
        self.chat_interface.initialize_chat()
        self.chat_interface.display_messages()
        
    def run(self):
        """Run application"""
        try:
            # Initialize app
            self.initialize()
            
            # Setup sidebar and get model choice
            model_choice = self.sidebar_manager.setup_sidebar()
            
            # Setup chat interface
            self.setup_chat_interface(model_choice)
            
            # Initialize AI model
            retriever = (get_openai_retriever() if model_choice == "GitHub AI"
                        else get_ollama_retriever())
            agent_executor = (get_openai_agent(retriever) if model_choice == "GitHub AI"
                            else get_ollama_agent(retriever))
            
            # Handle user input
            self.chat_interface.handle_user_input(agent_executor)
            
        except Exception as e:
            st.error("·ª®ng d·ª•ng g·∫∑p l·ªói. Vui l√≤ng th·ª≠ l·∫°i sau.")
            logger.error(f"Application error: {str(e)}")

if __name__ == "__main__":
    app = App()
    app.run() 
